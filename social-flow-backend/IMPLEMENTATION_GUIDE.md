# ðŸŽ¯ Implementation Guide - Next Steps

## Quick Start: Integrating Enhanced Infrastructure

### Step 1: Review What's Been Created

Three new core files have been created with production-grade infrastructure:

1. **`app/core/config_enhanced.py`** - Comprehensive configuration (700+ lines)
2. **`app/core/database_enhanced.py`** - Advanced database management (550+ lines)
3. **`app/core/redis_enhanced.py`** - Redis infrastructure (750+ lines)

### Step 2: Update Main Application

Replace imports in `app/main.py`:

```python
# OLD imports
from app.core.config import settings
from app.core.database import init_db
from app.core.redis import init_redis

# NEW imports
from app.core.config_enhanced import settings
from app.core.database_enhanced import db_manager
from app.core.redis_enhanced import redis_manager

# Update lifespan function
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Enhanced application lifespan."""
    # Startup
    setup_logging()
    
    await db_manager.initialize()
    await redis_manager.initialize()
    
    # Health checks
    db_health = await db_manager.health_check()
    redis_health = await redis_manager.health_check()
    
    logger.info(f"DB Health: {db_health}")
    logger.info(f"Redis Health: {redis_health}")
    
    yield
    
    # Shutdown
    await db_manager.close()
    await redis_manager.close()
```

### Step 3: Update Dependencies in Endpoints

```python
# In your endpoint files (e.g., app/api/v1/endpoints/users.py)

# OLD
from app.core.database import get_db

# NEW - choose based on operation type
from app.core.database_enhanced import get_db, get_db_readonly
from app.core.redis_enhanced import redis_manager, cache_result

# For read operations (use replicas)
@router.get("/users/{user_id}")
async def get_user(
    user_id: str, 
    db: AsyncSession = Depends(get_db_readonly)  # Uses read replica
):
    user = await db.get(User, user_id)
    return user

# For write operations (use primary)
@router.post("/users")
async def create_user(
    user_data: UserCreate,
    db: AsyncSession = Depends(get_db)  # Uses primary database
):
    user = User(**user_data.dict())
    db.add(user)
    await db.commit()
    return user
```

### Step 4: Add Caching to Expensive Operations

```python
from app.core.redis_enhanced import cache_result

# Automatic caching with decorator
@cache_result(ttl=600, key_prefix="user:profile")
async def get_user_profile_with_stats(user_id: str, db: AsyncSession):
    """This function's results will be cached for 10 minutes."""
    user = await db.get(User, user_id)
    # Expensive calculations...
    stats = await calculate_user_stats(user_id, db)
    return {**user.dict(), "stats": stats}

# Use in endpoint
@router.get("/users/{user_id}/profile")
async def get_profile(user_id: str, db: AsyncSession = Depends(get_db_readonly)):
    return await get_user_profile_with_stats(user_id, db)
```

### Step 5: Implement Rate Limiting

```python
from app.core.redis_enhanced import RateLimiter
from fastapi import HTTPException

rate_limiter = RateLimiter(redis_manager.get_client())

@router.post("/posts")
async def create_post(
    post_data: PostCreate,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    # Rate limit: 10 posts per hour per user
    allowed = await rate_limiter.is_allowed(
        f"create_post:{current_user.id}",
        max_requests=10,
        window_seconds=3600
    )
    
    if not allowed:
        raise HTTPException(
            status_code=429,
            detail="Rate limit exceeded. Maximum 10 posts per hour."
        )
    
    # Create post...
    post = Post(**post_data.dict(), user_id=current_user.id)
    db.add(post)
    await db.commit()
    return post
```

### Step 6: Use Distributed Locking for Critical Sections

```python
from app.core.redis_enhanced import redis_manager

@router.post("/videos/{video_id}/process")
async def process_video(video_id: str):
    # Prevent duplicate processing
    lock = await redis_manager.acquire_lock(
        f"process:video:{video_id}",
        timeout=300,  # 5 minutes
        blocking=False
    )
    
    if not lock:
        raise HTTPException(
            status_code=409,
            detail="Video is already being processed"
        )
    
    try:
        # Process video...
        await encode_video(video_id)
        return {"status": "processing_started"}
    finally:
        await redis_manager.release_lock(lock)
```

### Step 7: Update Environment Variables

Update your `.env` file to use new settings:

```env
# App
ENVIRONMENT=development
DEBUG=True
PROJECT_NAME=Social Flow

# Database (with sharding support)
DATABASE_URL=postgresql+asyncpg://postgres:password@localhost:5432/social_flow
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=40

# Optional: Read replicas for scaling
DATABASE_READ_REPLICAS=postgresql+asyncpg://postgres:password@replica1:5432/social_flow,postgresql+asyncpg://postgres:password@replica2:5432/social_flow

# Optional: Database sharding
DB_SHARDING_ENABLED=false
DB_SHARD_COUNT=4

# Redis (with cluster support)
REDIS_URL=redis://localhost:6379/0
REDIS_MAX_CONNECTIONS=50

# Optional: Redis cluster
REDIS_CLUSTER_ENABLED=false
REDIS_CLUSTER_NODES=redis1:6379,redis2:6379,redis3:6379

# Cache TTLs
CACHE_TTL_DEFAULT=300
CACHE_TTL_USER_PROFILE=600
CACHE_TTL_VIDEO_METADATA=1800
CACHE_TTL_FEED=60

# AWS Services
AWS_REGION=us-east-1
AWS_S3_BUCKET_VIDEOS=social-flow-videos
AWS_S3_BUCKET_IMAGES=social-flow-images
AWS_CLOUDFRONT_DOMAIN=d1234567890.cloudfront.net

# Security
SECRET_KEY=your-secret-key-change-in-production
ACCESS_TOKEN_EXPIRE_MINUTES=30
REFRESH_TOKEN_EXPIRE_DAYS=7

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_PER_MINUTE=60
RATE_LIMIT_PER_HOUR=1000

# Feature Flags
FEATURE_LIVE_STREAMING=true
FEATURE_TWO_FACTOR_AUTH=true
FEATURE_SOCIAL_LOGIN=true

# Observability
LOG_LEVEL=INFO
METRICS_ENABLED=true
TRACING_ENABLED=false
```

---

## Recommended Development Workflow

### 1. Test Enhanced Infrastructure

Create a test script (`test_infrastructure.py`):

```python
import asyncio
from app.core.database_enhanced import db_manager
from app.core.redis_enhanced import redis_manager

async def test_infrastructure():
    """Test enhanced infrastructure."""
    print("Testing infrastructure...")
    
    # Initialize
    await db_manager.initialize()
    await redis_manager.initialize()
    
    # Test database
    print("\n=== Database Health ===")
    db_health = await db_manager.health_check()
    for conn_name, is_healthy in db_health.items():
        status = "âœ“" if is_healthy else "âœ—"
        print(f"{status} {conn_name}: {'OK' if is_healthy else 'FAIL'}")
    
    # Test Redis
    print("\n=== Redis Health ===")
    redis_health = await redis_manager.health_check()
    status = "âœ“" if redis_health else "âœ—"
    print(f"{status} Redis: {'OK' if redis_health else 'FAIL'}")
    
    # Test caching
    print("\n=== Cache Operations ===")
    await redis_manager.set("test:key", "test_value", ttl=60)
    value = await redis_manager.get("test:key")
    print(f"âœ“ Cache set/get: {value}")
    
    # Test rate limiting
    print("\n=== Rate Limiting ===")
    from app.core.redis_enhanced import RateLimiter
    rate_limiter = RateLimiter(redis_manager.get_client())
    
    for i in range(5):
        allowed = await rate_limiter.is_allowed("test:user", 3, 60)
        status = "allowed" if allowed else "blocked"
        print(f"  Request {i+1}: {status}")
    
    # Cleanup
    await db_manager.close()
    await redis_manager.close()
    
    print("\nâœ“ All tests passed!")

if __name__ == "__main__":
    asyncio.run(test_infrastructure())
```

Run it:
```bash
python test_infrastructure.py
```

### 2. Gradually Migrate Existing Code

**Strategy**: Migrate module by module, not all at once.

**Week 1**: Auth module
```python
# Update app/auth/api/auth.py
from app.core.database_enhanced import get_db
from app.core.redis_enhanced import redis_manager, RateLimiter

# Add rate limiting to login
# Add caching to user profiles
# Test thoroughly
```

**Week 2**: Videos module
```python
# Update app/videos/api/videos.py
from app.core.database_enhanced import get_db, get_db_readonly
from app.core.redis_enhanced import cache_result

# Add read replica for video listings
# Cache video metadata
# Test thoroughly
```

**Week 3**: Posts and social features
**Week 4**: Payments and ads
**Week 5**: ML and analytics

### 3. Monitor Performance

Add this health endpoint to `app/main.py`:

```python
@app.get("/health/detailed")
async def detailed_health():
    """Detailed health check with infrastructure status."""
    db_health = await db_manager.health_check()
    redis_health = await redis_manager.health_check()
    
    return {
        "status": "healthy" if all(db_health.values()) and redis_health else "unhealthy",
        "database": db_health,
        "redis": redis_health,
        "timestamp": datetime.utcnow().isoformat()
    }
```

---

## Common Patterns

### Pattern 1: User Profile with Caching

```python
from app.core.redis_enhanced import cache_result
from app.core.database_enhanced import get_db_readonly

@cache_result(ttl=600, key_prefix="user")
async def get_user_with_stats(user_id: str, db: AsyncSession):
    """Get user with computed stats (cached for 10 min)."""
    user = await db.get(User, user_id)
    if not user:
        return None
    
    # Expensive aggregations
    video_count = await db.scalar(
        select(func.count(Video.id)).where(Video.user_id == user_id)
    )
    follower_count = await db.scalar(
        select(func.count(Follow.id)).where(Follow.following_id == user_id)
    )
    
    return {
        **user.__dict__,
        "video_count": video_count,
        "follower_count": follower_count
    }

@router.get("/users/{user_id}")
async def get_user_profile(
    user_id: str,
    db: AsyncSession = Depends(get_db_readonly)
):
    user = await get_user_with_stats(user_id, db)
    if not user:
        raise HTTPException(404, "User not found")
    return user
```

### Pattern 2: Feed with Pagination and Caching

```python
@router.get("/feed")
async def get_feed(
    cursor: Optional[str] = None,
    limit: int = Query(20, le=100),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db_readonly)
):
    # Try cache first (short TTL for feeds)
    cache_key = f"feed:{current_user.id}:{cursor}:{limit}"
    cached_feed = await redis_manager.get(cache_key)
    if cached_feed:
        return cached_feed
    
    # Build query
    query = (
        select(Post)
        .join(Follow, Follow.following_id == Post.user_id)
        .where(Follow.follower_id == current_user.id)
        .order_by(Post.created_at.desc())
        .limit(limit + 1)
    )
    
    if cursor:
        # Decode cursor to get last post ID and timestamp
        last_id, last_timestamp = decode_cursor(cursor)
        query = query.where(
            or_(
                Post.created_at < last_timestamp,
                and_(
                    Post.created_at == last_timestamp,
                    Post.id < last_id
                )
            )
        )
    
    result = await db.execute(query)
    posts = result.scalars().all()
    
    # Prepare response
    has_more = len(posts) > limit
    if has_more:
        posts = posts[:-1]
    
    next_cursor = None
    if has_more and posts:
        last_post = posts[-1]
        next_cursor = encode_cursor(last_post.id, last_post.created_at)
    
    response = {
        "posts": [post.dict() for post in posts],
        "next_cursor": next_cursor,
        "has_more": has_more
    }
    
    # Cache for 1 minute
    await redis_manager.set(cache_key, response, ttl=60)
    
    return response
```

### Pattern 3: View Count Tracking

```python
from app.core.redis_enhanced import redis_manager

async def track_video_view(video_id: str, user_id: str):
    """Track video view with Redis batching."""
    # Check if user already viewed recently (prevent double counting)
    view_key = f"view:recent:{video_id}:{user_id}"
    already_viewed = await redis_manager.exists(view_key)
    
    if already_viewed:
        return False
    
    # Mark as viewed (expires after 24 hours)
    await redis_manager.set(view_key, "1", ttl=86400)
    
    # Increment buffered count
    buffer_key = f"view:buffer:{video_id}"
    await redis_manager.incr(buffer_key)
    
    # Set TTL on first increment
    if await redis_manager.get(buffer_key) == "1":
        await redis_manager.expire(buffer_key, 300)  # Flush after 5 minutes
    
    return True

# Background task to flush view counts to database
async def flush_view_counts():
    """Flush buffered view counts to database (run every minute)."""
    pattern = "view:buffer:*"
    # Get all buffered videos (simplified, use scan in production)
    keys = await redis_manager._redis.keys(pattern)
    
    if not keys:
        return
    
    async with db_manager.session() as db:
        for key in keys:
            video_id = key.decode().split(":")[-1]
            count = int(await redis_manager.get(key) or 0)
            
            if count > 0:
                # Update database
                await db.execute(
                    update(Video)
                    .where(Video.id == video_id)
                    .values(view_count=Video.view_count + count)
                )
                
                # Clear buffer
                await redis_manager.delete(key)
        
        await db.commit()
```

### Pattern 4: Distributed Lock for Processing

```python
async def process_video_with_lock(video_id: str):
    """Process video with distributed lock to prevent duplicates."""
    lock_name = f"process:video:{video_id}"
    lock = await redis_manager.acquire_lock(
        lock_name,
        timeout=600,  # 10 minutes max processing time
        blocking=False
    )
    
    if not lock:
        # Another worker is already processing
        logger.info(f"Video {video_id} is already being processed")
        return {"status": "already_processing"}
    
    try:
        logger.info(f"Starting processing for video {video_id}")
        
        # Update status
        async with db_manager.session() as db:
            await db.execute(
                update(Video)
                .where(Video.id == video_id)
                .values(status="processing")
            )
            await db.commit()
        
        # Actual processing (can take several minutes)
        result = await encode_video(video_id)
        
        # Update status
        async with db_manager.session() as db:
            await db.execute(
                update(Video)
                .where(Video.id == video_id)
                .values(
                    status="completed",
                    hls_playlist_url=result["hls_url"],
                    thumbnail_url=result["thumbnail_url"]
                )
            )
            await db.commit()
        
        logger.info(f"Completed processing for video {video_id}")
        return {"status": "completed", "result": result}
    
    except Exception as e:
        logger.error(f"Error processing video {video_id}: {e}")
        
        # Update status to failed
        async with db_manager.session() as db:
            await db.execute(
                update(Video)
                .where(Video.id == video_id)
                .values(status="failed", error_message=str(e))
            )
            await db.commit()
        
        raise
    
    finally:
        # Always release lock
        await redis_manager.release_lock(lock)
```

---

## Testing Enhanced Infrastructure

### Unit Tests

```python
# tests/unit/test_redis_enhanced.py
import pytest
from app.core.redis_enhanced import redis_manager, RateLimiter

@pytest.mark.asyncio
async def test_redis_basic_operations():
    await redis_manager.initialize()
    
    # Test set/get
    await redis_manager.set("test:key", "value", ttl=60)
    value = await redis_manager.get("test:key")
    assert value == "value"
    
    # Test delete
    await redis_manager.delete("test:key")
    value = await redis_manager.get("test:key")
    assert value is None
    
    await redis_manager.close()

@pytest.mark.asyncio
async def test_rate_limiter():
    await redis_manager.initialize()
    rate_limiter = RateLimiter(redis_manager.get_client())
    
    # Should allow first 3 requests
    for i in range(3):
        allowed = await rate_limiter.is_allowed("test:user", 3, 60)
        assert allowed is True
    
    # Should block 4th request
    allowed = await rate_limiter.is_allowed("test:user", 3, 60)
    assert allowed is False
    
    await redis_manager.close()
```

### Integration Tests

```python
# tests/integration/test_database_enhanced.py
import pytest
from app.core.database_enhanced import db_manager
from app.models import User

@pytest.mark.asyncio
async def test_database_operations():
    await db_manager.initialize()
    
    # Test write to primary
    async with db_manager.session() as db:
        user = User(username="testuser", email="test@example.com")
        db.add(user)
        await db.commit()
        user_id = user.id
    
    # Test read from replica
    async with db_manager.session(readonly=True) as db:
        user = await db.get(User, user_id)
        assert user.username == "testuser"
    
    # Cleanup
    async with db_manager.session() as db:
        await db.delete(user)
        await db.commit()
    
    await db_manager.close()
```

---

## Deployment Checklist

### Before Deploying

- [ ] Update `.env` with production values
- [ ] Test all endpoints with new infrastructure
- [ ] Run full test suite
- [ ] Check database migrations
- [ ] Verify Redis connectivity
- [ ] Test AWS service connections
- [ ] Review security settings
- [ ] Check rate limits
- [ ] Verify logging configuration
- [ ] Test health endpoints

### Production Environment Variables

```env
ENVIRONMENT=production
DEBUG=False

# Use actual production values
DATABASE_URL=postgresql+asyncpg://user:pass@prod-db.amazonaws.com:5432/socialflow
REDIS_URL=redis://prod-redis.amazonaws.com:6379/0

AWS_REGION=us-east-1
AWS_S3_BUCKET_VIDEOS=socialflow-videos-prod
AWS_CLOUDFRONT_DOMAIN=dxyz123.cloudfront.net

SECRET_KEY=<generate-strong-secret-key>
SENTRY_DSN=https://your-sentry-dsn

RATE_LIMIT_PER_MINUTE=100
RATE_LIMIT_PER_HOUR=5000
```

### Monitoring

- Set up CloudWatch dashboards
- Configure alerts for errors
- Monitor database connections
- Track Redis memory usage
- Monitor API response times
- Set up APM (Application Performance Monitoring)

---

## Support

For questions:
- Review `TRANSFORMATION_SUMMARY.md` for architecture overview
- Check `TRANSFORMATION_CHANGELOG.md` for detailed changes
- Refer to inline code documentation
- Contact: Nirmal Meena (+91 93516 88554)

---

**Remember**: Start small, test thoroughly, migrate gradually. The enhanced infrastructure is designed to be backward compatible, so you can migrate at your own pace! ðŸš€
